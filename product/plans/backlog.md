# Product Backlog

## High Priority Enhancements

### AI-Driven Sentiment, Engagement & Conversion Assessment

**Status:** Backlog  
**Priority:** High  
**Effort:** Medium

**Problem:**
Current sentiment, engagement, and conversion metrics are generated by simplistic keyword-based analysis in `backfill_packager.py`, resulting in:

- 100% "Neutral" sentiment (no variation)
- 98.5% "Low" conversion (minimal variation)
- Misleading dashboard metrics that suggest sophisticated analysis when it's just broken keyword counting

**Solution:**
Replace keyword-based analysis with AI-driven assessment during the backfill process:

1. **Design AI Prompt** for sentiment/engagement/conversion analysis that:

   - Analyzes the complete audit rationale and evidence
   - Considers persona-specific context and priorities
   - Provides nuanced assessment beyond simple positive/negative/neutral
   - Evaluates engagement potential from persona perspective
   - Assesses conversion likelihood based on business context

2. **Integration Approach:**

   - Add AI assessment step to `backfill_packager.py`
   - Use existing AI interface (`ai_interface.py`)
   - Process audit results through dedicated prompt
   - Generate meaningful sentiment/engagement/conversion values

3. **Expected Outcomes:**
   - Meaningful variation in sentiment scores
   - Persona-relevant engagement assessments
   - Business-context-aware conversion likelihood
   - Dashboard metrics that provide actual insights

**Acceptance Criteria:**

- [ ] AI prompt designed and tested for sentiment/engagement/conversion assessment
- [ ] Integration implemented in backfill process
- [ ] Dashboard shows meaningful variation in metrics
- [ ] Metrics correlate with audit scores and persona priorities
- [ ] Documentation updated with new assessment methodology

**Dependencies:**

- Existing AI interface and audit pipeline
- Dashboard metrics removal/replacement coordination

---

### Strategic Assessment Algorithm Redesign

**Status:** Backlog  
**Priority:** High  
**Effort:** Medium-High

**Problem:**
Current strategic assessment ("Are we distinct?", "Are we resonating?", "Are we converting?") has fundamental flaws:

- **Distinctiveness & Converting use identical calculations** (both = average page scores = 5.8/10)
- **Resonating shows no real data** (100% neutral sentiment, 0% positive/negative)
- **Users see duplicate metrics** disguised as different strategic insights
- **No meaningful differentiation** between the three strategic questions

**Root Cause Analysis:**

1. Distinctiveness and conversion both calculate `df[score_col].mean()` - literally the same algorithm
2. Sentiment data pipeline produces only "Neutral" values (broken/missing sentiment analysis)
3. No distinct data sources mapped to each strategic question

**Solution:**
Redesign each strategic assessment with distinct algorithms using existing unified CSV columns:

#### üîç **Distinctiveness Algorithm** - "Are we distinct?"

**Goal:** Measure brand differentiation and positioning clarity

**Data Sources:**

- `first_impression` (0-10) - Initial brand impact
- `brand_percentage` (0-100) - Brand-focused content ratio
- `language_tone_feedback` (0-10) - Tone uniqueness assessment

**Algorithm:**

```python
df['distinct_score'] = (
    df['first_impression'] * 0.4 +
    (df['brand_percentage'] / 10) * 0.3 +
    df['language_tone_feedback'] * 0.3
)
distinctiveness = df.groupby('page_id')['distinct_score'].mean()
```

**Thresholds:**

- ‚â•7.0 ‚Üí "Distinct" (‚úÖ)
- 5.0-6.9 ‚Üí "Partially" (‚ö†Ô∏è)
- <5.0 ‚Üí "Not Distinct" (‚ùå)

#### üí≠ **Resonance Algorithm** - "Are we resonating?"

**Goal:** Capture emotional engagement and user connection

**Data Sources:**

- `sentiment_numeric` (-10 to +10) - Emotional response
- `engagement_numeric` (0-10) - User engagement level
- `success_flag` (boolean) - High-performing page indicator
- `overall_sentiment` (backup string values)

**Algorithm:**

```python
# Convert sentiment to 0-10 scale
df['sent_scale'] = (df['sentiment_numeric'] + 10) / 2

df['resonate_score'] = (
    df['sent_scale'] * 0.5 +
    df['engagement_numeric'] * 0.4 +
    df['success_flag'].astype(int) * 10 * 0.1
)
resonance = df.groupby('page_id')['resonate_score'].mean()
```

**Thresholds:**

- ‚â•7.0 ‚Üí "Resonating" (‚úÖ)
- 3.0-6.9 ‚Üí "Mixed Response" (‚ö†Ô∏è)
- <3.0 ‚Üí "Not Resonating" (‚ùå)

#### üí∞ **Conversion Algorithm** - "Are we converting?"

**Goal:** Evaluate lead generation and sales readiness

**Data Sources:**

- `conversion_numeric` (0-10) - Conversion likelihood
- `trust_credibility_assessment` (0-10) - Trust signals
- `performance_percentage` (0-100) - Overall page performance
- `critical_issue_flag` (boolean) - Conversion blockers

**Algorithm:**

```python
df['convert_score'] = (
    df['conversion_numeric'] * 0.5 +
    df['trust_credibility_assessment'] * 0.3 +
    (df['performance_percentage'] / 10) * 0.2
)

# Penalize critical issues
df.loc[df['critical_issue_flag'] == True, 'convert_score'] -= 2
df['convert_score'] = df['convert_score'].clip(0, 10)

conversion = df.groupby('page_id')['convert_score'].mean()
```

**Thresholds:**

- ‚â•7.0 ‚Üí "High Conversion" (‚úÖ)
- 5.0-6.9 ‚Üí "Medium Conversion" (‚ö†Ô∏è)
- <5.0 ‚Üí "Low Conversion" (‚ùå)

**Implementation Plan:**

1. **Data Pipeline Updates** (`packager.py`/`data_gateway.py`):

   - Normalize `language_tone_feedback` to 0-10 numeric scale
   - Populate missing `sentiment_numeric` and `engagement_numeric` columns
   - Convert `trust_credibility_assessment` from text to 0-10 numeric
   - Ensure `critical_issue_flag` stored as boolean
   - Update `unified_csv_columns.yaml` with new/modified columns

2. **Calculator Methods** (`metrics_calculator.py`):

   - Replace `calculate_tier_performance()` logic for distinctiveness
   - Fix `calculate_sentiment_metrics()` to use new resonance algorithm
   - Redesign `calculate_conversion_readiness()` with new conversion algorithm
   - Add proper error handling for missing data

3. **Dashboard Updates** (`brand_health_command_center.py`):

   - Update strategic assessment section to use new methods
   - Fix duplicate metric display issue
   - Add methodology tooltips/expandable sections
   - Update status thresholds and color coding

4. **Data Backfill**:
   - Re-run `multi_persona_packager.py` to generate updated unified dataset
   - Run integrity checker to validate new column structure
   - Test with existing audit data to ensure backward compatibility

**Acceptance Criteria:**

- [ ] Each strategic question uses distinct data sources and algorithms
- [ ] No duplicate calculations between distinctiveness and conversion
- [ ] Sentiment/resonance shows meaningful variation (not 100% neutral)
- [ ] All three metrics use consistent 0-10 scale with clear thresholds
- [ ] Dashboard displays differentiated insights for each strategic question
- [ ] Methodology documentation added to UI (tooltips/help sections)
- [ ] Integrity checker passes with 0 column mismatch issues
- [ ] Backward compatibility maintained with existing audit data

**Dependencies:**

- AI-Driven Sentiment Assessment (above) - needed for meaningful sentiment_numeric values
- Existing unified CSV pipeline and dashboard infrastructure
- Column standardization across audit methodology

**Risk Mitigation:**

- Implement with feature flags to allow rollback to current system
- Validate algorithms with sample data before full deployment
- Maintain current metrics as backup during transition period

---

## Future Enhancements

_Additional backlog items to be added as needed_
